---
title: "Project 2 Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Group Members (names and EIDs): Ria Bhatia (rnb982) & Ethan Benson (erb2994)




## Introduction

*In order to learn more about the relationships in the data, we used multiple exploratory data analysis techniques. First, we conducted a PCA test, which was then used to product a rotation plot, eigenvalues plot, and a scatter plot.*



```{r}
# Data exploration
library(tidyverse)
library(broom)

dat <- read_csv("pm25_data.csv.gz")


dat_clean <- dat |>
  select(-id) |>
  select(where(is.numeric))

# Scatter plot of CMAQ vs AOD
dat_clean |> 
  ggplot(aes(CMAQ, aod)) +
  geom_point(color = "darkgreen") +
  theme_classic() +
  labs(
    title = "CMAQ vs AOD",
    x = "CMAQ",
    y = "AOD"
  ) +
  theme(
    plot.title = element_text(face = "bold")
  )

# PCA
pca_fit <- dat_clean |> 
  select(where(is.numeric)) |> 
  scale() |> 
  prcomp()
pca_fit

# Rotation plot
arrow_style <- arrow(
  angle = 20, length = grid::unit(8, "pt"),
  ends = "first", type = "closed"
)

pca_fit |> 
  tidy(matrix = "rotation") |>   
  pivot_wider(
    names_from = "PC", values_from = "value",
    names_prefix = "PC"
  ) |> 
  ggplot(aes(PC1, PC2)) +
  geom_segment(
    xend = 0, yend = 0,
    arrow = arrow_style
  ) +
  geom_text(aes(label = column)) +
  coord_fixed() +
  labs(
    title = "Rotation Plot of PC 1 vs PC 2"
  ) + xlim(-.4, .6) 

# Eigenvalues plot
pca_fit |> 
  tidy(matrix = "eigenvalues") |> 
  ggplot(aes(PC, percent)) +
  geom_col() +
  scale_x_continuous() +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(
    title = "Eigenvalues Plot of Principle Components"
  )

# Scatter plot
pca_fit |> 
  augment(dat_clean) |> 
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, color = dat_clean$value)) +
  geom_point() +
  labs(
    title = "PC 2 vs PC 1 by Average PM2.5 Levels",
    x = "PC 1",
    y = "PC 2"
  ) +
  theme_grey() +
  theme(
    plot.title = element_text(face = "bold")
  ) +
  coord_fixed()  +
  scale_color_gradient(low = "blue", high = "red", breaks = c(0, 10, 20, 30))

```


## Data Wrangling

```{r}

library(tidymodels)

dat <- read_csv("pm25_data.csv.gz")

dat_split <- initial_split(dat_clean)

#Split into train and test
dat_train <- training(dat_split)
dat_test <- testing(dat_split)
```

```{r}
#Linear Regression Model
rec <- dat_train |>
    recipe(value ~ .)

model <- linear_reg() |> 
    set_engine("lm") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

folds <- vfold_cv(dat_train, v = 10)

lin_final <- wf |> 
    fit_resamples(resamples = folds)

res_lin <- lin_final |> 
    collect_metrics()

```

```{r}

# Neural Net (Multilayer Perceptron)
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize() 

model <- mlp(hidden_units = tune(), penalty = tune(),
             epochs = tune()) |> 
  set_engine("nnet") |> 
  set_mode("regression")

wf <- workflow() |> 
    add_model(model) |> 
    add_recipe(rec)

mlp_grid <- expand.grid(
  hidden_units = c(5, 10, 15),
  penalty = c(0, 0.01, 0.1),
  epochs = c(10, 50, 100)
)

folds <- vfold_cv(dat_train, v = 10)

res <- tune_grid(wf, resamples = folds, grid = mlp_grid)

res |> 
    show_best(metric = "rmse")

## Fit the best model obtained from tuning
model <- mlp(hidden_units = 10, penalty = .1,
             epochs = 50) |> 
  set_engine("nnet") |> 
  set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Neural Net Final model fit
NN_final <- wf |> 
    last_fit(split = dat_split)

res_NN <- NN_final |> 
    collect_metrics()

```


```{r}

# kNN

rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize()

## Tune for the optimal number of neighbors
model <- nearest_neighbor(neighbors = tune("k")) |> 
    set_engine("kknn") |> 
    set_mode("regression")
wf <- workflow() |> 
    add_model(model) |> 
    add_recipe(rec)

folds <- vfold_cv(dat_train, v = 10)

res <- tune_grid(wf, resamples = folds,
                 grid =tibble(k = c(10, 12, 13, 15, 17, 18, 20, 25)))

res |> 
    show_best(metric = "rmse")

# Evaluate model with best k = 12
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize()

model <- nearest_neighbor(neighbors = 12) |> 
    set_engine("kknn") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Final model fit
kNN_final <- wf |> 
    last_fit(split = dat_split)

res_kNN <- kNN_final |> 
    collect_metrics()

```

```{r}
library(ranger)
# Random Forest
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize() 

model <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Fit model over grid of tuning parameters
res <- tune_grid(wf, resamples = folds, 
                 grid = expand.grid(mtry = c(1, 2, 5),
                                    min_n = c(3, 5)))

res |> 
    show_best(metric = "rmse")


## Fit the best model obtained from tuning
model <- rand_forest(mtry = 5,
                     min_n = 3) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Final model fit
randFor_final <- wf |> 
    last_fit(split = dat_split)

res_rf <-randFor_final |> 
    collect_metrics()

```


## Results

```{r}
# Output all RMSE for each model - Ethan
# Linear Model Metrics
res_lin

# Neural net Metrics
res_NN

# kNN Metrics
res_kNN

# Random Forest Metrics
res_rf

# Prep dat_test
dat_test <- rec |> 
    prep() |> 
    bake(new_data = testing(dat_split))

# Scatter plot of predicted vs observed data points
randFor_final |> 
    extract_fit_parsnip() |> 
    augment(new_data = dat_test) |> 
    select(value, .pred) |> 
    ggplot(aes(.pred, value)) + 
    geom_point(alpha = 1/10)
```


```{r}

# Any code for primary questions

```
## Discussion