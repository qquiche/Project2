---
title: "Project 2 Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Group Members (names and EIDs): Ria Bhatia (rnb982) & Ethan Benson (erb2994)




## Introduction

*For our comparison, we chose the following modeling approaches: linear regression, neural net (multilayer perceptron), kNN, and random forest. With these models, we used all of the available predictors, so we could truly understand the relationship between each predictor and the observations. In order to learn more about the relationships in the data, we used multiple exploratory data analysis techniques. First, we created a scatter plot to understand the relationship between CMAQ and AOD. Second, we conducted a PCA test, which was then used to produce a rotation plot, eigenvalues plot, and scatter plot to understand which variables might provide the strongest relations to average PM2.5 levels. After creating the models and conducting exploratory analysis, we hypothesized that either neural nets or random forest would be the best model. This would be because random forest incorporates ensembling decision trees, which usually leads to better results and neural nets has a lot of hyper parameters we can tune. Such models work great for a lots of input data. We believed linear and KNN models would perform worse as KNN would suffer form the curse of dimensionality due to the large amount of variables and linear models are too simplistic for the possible complex relationships between so many variables.*


## Data Wrangling
```{r}
# Reading in the libraries and data set
library(tidyverse)
library(broom)
library(tidymodels)

dat <- read_csv("pm25_data.csv.gz")

dat_split <- initial_split(dat)

#Split into train and test and saving the raw data for questions
dat_train_raw <- training(dat_split)
dat_test_raw <- testing(dat_split)

# Cleaning the data set
dat_clean <- dat |> 
  select(-id, fips) |>
  select(where(is.numeric))

dat_train <- dat_train_raw |>
  select(-id, -fips) |>
  select(where(is.numeric))

dat_test <- dat_test_raw |>
  select(-id, -fips) |>
  select(where(is.numeric))

```
*In the above code chunk, we loaded the libraries and read in the data. We split the data into the train and test data sets, and then proceeded to clean the data sets.*

```{r}
# Data exploration

# Scatter plot of CMAQ vs AOD
dat_clean |> 
  ggplot(aes(CMAQ, aod)) +
  geom_point(color = "darkgreen") +
  theme_classic() +
  labs(
    title = "CMAQ vs AOD",
    x = "CMAQ",
    y = "AOD"
  ) +
  theme(
    plot.title = element_text(face = "bold")
  )
```
*In the above visualization, we are able to understand the relationship between CMAQ and AOD. The two variables appear to have a moderately positive relationship, but there are many outliers also present.*

```{r}
# Data exploration continued - PCA

# PCA
pca_fit <- dat_clean |> 
  select(where(is.numeric)) |> 
  scale() |> 
  prcomp()
pca_fit

# Rotation plot
arrow_style <- arrow(
  angle = 20, length = grid::unit(8, "pt"),
  ends = "first", type = "closed"
)

pca_fit |> 
  tidy(matrix = "rotation") |>   
  pivot_wider(
    names_from = "PC", values_from = "value",
    names_prefix = "PC"
  ) |> 
  ggplot(aes(PC1, PC2)) +
  geom_segment(
    xend = 0, yend = 0,
    arrow = arrow_style
  ) +
  geom_text(aes(label = column)) +
  coord_fixed() +
  labs(
    title = "Rotation Plot of PC 1 vs PC 2"
  ) + xlim(-.4, .6) 

# Eigenvalues plot
pca_fit |> 
  tidy(matrix = "eigenvalues") |> 
  ggplot(aes(PC, percent)) +
  geom_col() +
  scale_x_continuous() +
  scale_y_continuous(labels = scales::label_percent()) +
  labs(
    title = "Eigenvalues Plot of Principle Components"
  )

# Scatter plot
pca_fit |> 
  augment(dat_clean) |> 
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, color = dat_clean$value)) +
  geom_point() +
  labs(
    title = "PC 2 vs PC 1 by Average PM2.5 Levels",
    x = "PC 1",
    y = "PC 2"
  ) +
  theme_grey() +
  theme(
    plot.title = element_text(face = "bold")
  ) +
  coord_fixed()  +
  scale_color_gradient(low = "blue", high = "red", breaks = c(0, 10, 20, 30))

```
*We conducted a PCA analysis of the variables in the data set to understand the predictors. From the eigenvalues plot, we found that PC 1 is associated with about 35% variance and PC 2 is associated with about 10% variance. We visualized the variables in a rotation plot, where in comparison to the scatter plot, we can understand the variation in the variables by the PM2.5 levels.*



```{r}
#Linear Regression Model
rec <- dat_train |>
    recipe(value ~ .)

model <- linear_reg() |> 
    set_engine("lm") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

folds <- vfold_cv(dat_train, v = 10)

lin_final <- wf |> 
    fit_resamples(resamples = folds)

res_lin <- lin_final |> 
    collect_metrics()

```

```{r}

# Neural Net (Multilayer Perceptron)
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize() 

model <- mlp(hidden_units = tune(), penalty = tune(),
             epochs = tune()) |> 
  set_engine("nnet") |> 
  set_mode("regression")

wf <- workflow() |> 
    add_model(model) |> 
    add_recipe(rec)

mlp_grid <- expand.grid(
  hidden_units = c(5, 10, 15),
  penalty = c(0, 0.01, 0.1),
  epochs = c(10, 50, 100)
)

folds <- vfold_cv(dat_train, v = 10)

res <- tune_grid(wf, resamples = folds, grid = mlp_grid)

res |> 
    show_best(metric = "rmse")

## Fit the best model obtained from tuning
model <- mlp(hidden_units = 10, penalty = .1,
             epochs = 50) |> 
  set_engine("nnet") |> 
  set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Neural Net Final model fit
NN_final <- wf |> 
    last_fit(split = dat_split)

res_NN <- NN_final |> 
    collect_metrics()

```


```{r}

# kNN

rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize()

## Tune for the optimal number of neighbors
model <- nearest_neighbor(neighbors = tune("k")) |> 
    set_engine("kknn") |> 
    set_mode("regression")
wf <- workflow() |> 
    add_model(model) |> 
    add_recipe(rec)

folds <- vfold_cv(dat_train, v = 10)

res <- tune_grid(wf, resamples = folds,
                 grid =tibble(k = c(10, 12, 13, 15, 17, 18, 20, 25)))

res |> 
    show_best(metric = "rmse")

# Evaluate model with best k = 12
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize()

model <- nearest_neighbor(neighbors = 12) |> 
    set_engine("kknn") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Final model fit
kNN_final <- wf |> 
    last_fit(split = dat_split)

res_kNN <- kNN_final |> 
    collect_metrics()

```

```{r}
library(ranger)
# Random Forest
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize() 

model <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Fit model over grid of tuning parameters
res <- tune_grid(wf, resamples = folds, 
                 grid = expand.grid(mtry = c(1, 2, 5),
                                    min_n = c(3, 5)))

res |> 
    show_best(metric = "rmse")


## Fit the best model obtained from tuning
model <- rand_forest(mtry = 5,
                     min_n = 3) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Final model fit
randFor_final <- wf |> 
    last_fit(split = dat_split)

res_rf <-randFor_final |> 
    collect_metrics()

```


## Results

```{r}
# Output all RMSE for each model - Ethan
# Linear Model Metrics
res_lin

# Neural net Metrics
res_NN

# kNN Metrics
res_kNN

# Random Forest Metrics
res_rf

# Prep dat_test
dat_test <- rec |> 
    prep() |> 
    bake(new_data = testing(dat_split))

# Scatter plot of predicted vs observed data points
randFor_final |> 
    extract_fit_parsnip() |> 
    augment(new_data = dat_test) |> 
    select(value, .pred) |> 
    ggplot(aes(.pred, value)) + 
    geom_point(alpha = 1/10)
```


```{r}
# Code for primary questions

# Question 1: Geographic locations

library(rnaturalearth)

# Combine test data with predictions
aug_rf <- randFor_final |> 
    extract_fit_parsnip() |> 
    augment(new_data = dat_test_raw)
    
# Get avg residuals per state
state_resid <- aug_rf |>
  select(value, .pred, state, county, city) |>
  mutate(residuals = abs(value - .pred)) |>
  group_by(state) |>
  summarize(avg_resid = mean(residuals), count = n()) |>
  arrange(avg_resid) |>
  mutate(name = state) |>
  select(-state)

sf_us <- ne_states(
  country = "United States of America",
  returnclass='sf'
)
sf_us <- sf_us |>
  left_join(state_resid, by = "name")

# Map of US color map residuals
sf_us |>
  # exclude Alaska (US02), Hawaii (US15)
  filter(!code_local %in% c("US02", "US15")) |>
  ggplot() + geom_sf(aes(fill = avg_resid)) +
  labs(title = "Average Residuals by State") +
  scale_fill_continuous(
    name = "Average Residual"
  )

```


```{r}
# Question 2: Variables

# State specific numeric means
aug_rf |>
  mutate(residuals = abs(value - .pred)) |>
  select(-id, -fips) |>
  group_by(state) |>
  summarize(count = n(), across(where(is.numeric), mean, na.rm = TRUE)) |>
  arrange(residuals)

```


```{r}
# Question 3: CMAQ & AOD
library(ranger)
# Random Forest
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_rm(CMAQ, aod)

model <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Fit model over grid of tuning parameters
res <- tune_grid(wf, resamples = folds, 
                 grid = expand.grid(mtry = c(1, 2, 5),
                                    min_n = c(3, 5)))

res |> 
    show_best(metric = "rmse")


## Fit the best model obtained from tuning
model <- rand_forest(mtry = 5,
                     min_n = 3) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Final model fit
randFor_final <- wf |> 
    last_fit(split = dat_split)

res_rf <-randFor_final |> 
    collect_metrics()

res_rf

```

## Discussion

*To answer Question 1, we color map the average residuals of average PM2.5 levels across states to see where our model predicted the best geographically. From what we can see, the model performed th best in Southern/Mideast portion of the United State, as we have lower residual values in the South. We also performed well in states closer to the Great Lakes.The higher residuals where we performed the worst was the Western states. The distribution of environmental factors affecting PM2.5 levels might vary across different regions. For example, industrial activity, traffic density, and population density can all influence air pollution levels. If our model is better trained on regions with similar environmental factors to those in the Southern/Southeastern US and states near the Great Lakes, it might perform better there. Also, Our model might generalize better to regions with characteristics similar to those in the Southern/Southeastern US and states near the Great Lakes. If the Western states have significantly different characteristics in terms of environmental factors, demographics, or other variables that affect PM2.5 levels, our model might struggle to make accurate predictions there.*

*To answer Question 2, I grouped by state and summarized all the numeric values we used as variables by mean. From the data, there doesn't seem to be much similarity between the ones our model predicted better vs worse.In terms of regions, the model performed th best in Southern/Mideast portion of the United State and states closer to the Great Lakes. However looking at the map I noticed that states with a larger area have a higher average residual. One possible hypothesis that states that are smaller have a lot more common geographical and environmental similarities so there are more data points that support a prediction while larger states have more diversity and have sensor more spread apart among vastly different environments leading to more spread-apart data that is harder to find patterns in causing the model to perform poorly. So maybe including state sizing could help the model take the diversity of the state and state wide policy into account*

*To answer Question 3, we conducted a random forest model with the removal of the CMAQ and AOD variables. After the analysis, we found that the model works best when the variables are included (RMSE = 1.760811) rather than when not included (RMSE = 1.855263). To answer Question 4, it is notable that the data set does not include data from Alaska or Hawaii. We believe that the model would not perform as well due to Alaska and Hawaii both having significantly different environments compared to the rest of the states. On one hand, Alaska is much colder can the rest of the states, which can lead to different air quality patterns. On the other hand, Hawaii tends to have warmer weather conditions and is surrounded by a body of water, leading to differing air quality patterns as well.*

*In the process of conducting this project, we faced challenges in choosing appropriate models to conduct our analysis. For instance, in the beginning, we wanted to include a logistic regression model, which did not end up being feasible for our analysis. Overall, we learned that conducting a thorough exploratory analysis to understand our data set would, in the end, help us choose models to evaluate. In our case, we ended up choosing 4 specific models that we predicted would be best for the provided data set.*

*Our best and final prediction model is the random forest model. We predicted that the random forest model would work well with the data since it incorporates ensemble learning and works well with complex, nonlinear relationships. Overall, the random forest model helped us understand the relationships within the air quality data set, including the many predictors present.*

*We would like to thank both Professor Peng and TA Bose for their guidance throughout the semester, especially during lecture and lab. This project includes contributions from 2 groups members. Ethan made the following contributions to the project: producing the models and results, editing the introduction, answering the primary questions, and writing the discussion. At the same time, Ria made the following contributions to the project: writing the introduction and discussion, conducting the exploratory analysis, and answering the primary questions.*
