---
title: "Project 2 Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Group Members (names and EIDs): Ria Bhatia (rnb982) & Ethan Benson (erb2994)




## Introduction



```{r}
library(tidyverse)

# Data exploration

#scatter plot CMAQ and aod

# PCA

#Rotation plot

#scatter plot

```


## Data Wrangling

```{r}

library(tidymodels)

dat <- read_csv("pm25_data.csv.gz")


dat_clean <- dat |>
  select(-id) |>
  select(where(is.numeric))

dat_split <- initial_split(dat_clean)

#SPlit into train and test
dat_train <- training(dat_split)
dat_test <- testing(dat_split)

#Linear Regression Model
rec <- dat_train |>
    recipe(value ~ .)

model <- linear_reg() |> 
    set_engine("lm") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

folds <- vfold_cv(dat_train, v = 10)

res_lin <- wf |> 
    fit_resamples(resamples = folds)
res_lin |> 
    collect_metrics()

```

```{r}

# Logistical Regression Model - Ethan
```


```{r}

# kNN

rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize() 

## Tune for the optimal number of neighbors
model <- nearest_neighbor(neighbors = tune("k")) |> 
    set_engine("kknn") |> 
    set_mode("regression")
wf <- workflow() |> 
    add_model(model) |> 
    add_recipe(rec)
wf

folds <- vfold_cv(dat_train, v = 10)

res <- tune_grid(wf, resamples = folds,
                 grid = tibble(k = c(10, 12, 13, 15, 17, 18, 20, 25)))

res |> 
    collect_metrics()

res |> 
    collect_metrics() |> 
    filter(.metric == "rmse") |> 
    ggplot(aes(k, mean)) +
    geom_point() + 
    geom_line()

res |> 
    show_best(metric = "rmse")

# Evaluate model with best k = 12

model <- nearest_neighbor(neighbors = 12) |> 
    set_engine("kknn") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_model(model) |> 
    add_recipe(rec)
wf

folds <- vfold_cv(dat_train, v = 10)

res <- fit_resamples(wf, resamples = folds)

res |> 
    collect_metrics()

```

```{r}
library(ranger)
# Random Forest
rec <- dat_train |> 
    recipe(value ~ .) |> 
    step_normalize() 

model <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Fit model over grid of tuning parameters
res <- tune_grid(wf, resamples = folds, 
                 grid = expand.grid(mtry = c(1, 2, 5),
                                    min_n = c(3, 5)))
#res |> 
    #collect_metrics()

res |> 
    show_best(metric = "rmse")

#res |> 
    #show_best(metric = "rsq")


## Fit the best model obtained from tuning
model <- rand_forest(mtry = 5,
                     min_n = 3) |> 
    set_engine("ranger") |> 
    set_mode("regression")

wf <- workflow() |> 
    add_recipe(rec) |> 
    add_model(model)

## Final model fit
final <- wf |> 
    last_fit(split = dat_split)

final |> 
    collect_metrics()

```


## Results

```{r}
# Output all RMSE for each model - Ethan

# Predict using final model on dat_test


# Scatter plot of predicted vs observed data points

```


```{r}

# Any code for primary questions

```
## Discussion